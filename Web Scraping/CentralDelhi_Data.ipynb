{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utilities \n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from chromedriver_py import binary_path   \n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize Chrome WebDriver\n",
    "def initialize_chrome():\n",
    "    chrome_options = ChromeOptions()\n",
    "    chrome_options.add_argument('log-level=3')\n",
    "    chrome_options.add_argument('--ignore-certificate-errors-spki-list')\n",
    "    chrome_options.add_argument('--ignore-ssl-errors')\n",
    "    #chrome_options.add_argument('--headless')  # You can uncomment this if you want to run Chrome in headless mode\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "    driver.maximize_window()  # Maximize the window\n",
    "    return driver\n",
    "# Function to initialize Firefox WebDriver\n",
    "def initialize_firefox():\n",
    "    firefox_options = FirefoxOptions()\n",
    "    firefox_options.headless = True  # Run Firefox in headless mode\n",
    "    driver = webdriver.Firefox(options=firefox_options)\n",
    "    driver.maximize_window()  # Maximize the window\n",
    "    return driver\n",
    "# Function to initialize WebDriver\n",
    "def initialize_driver():\n",
    "    try:\n",
    "        driver = initialize_chrome()\n",
    "        print(\"Chrome WebDriver initialized successfully.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(\"Chrome WebDriver initialization failed:\", e)\n",
    "        print(\"Switching to chrome WebDriver...\")\n",
    "        return initialize_firefox()\n",
    "\n",
    "# Function to wait for element visibility\n",
    "def wait_for_element_visibility(driver, by, selector, timeout=20):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.visibility_of_element_located((by, selector))\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrome WebDriver initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "driver = initialize_driver()\n",
    "\n",
    "driver.get(\"https://dispute.delhigovt.nic.in/frm_searchdisputedproperty.aspx?var=wNXACD7M+tg=&Digest=HRB2a7xKMHtq3BA3FqvhhQ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_page(driver): \n",
    "    # Wait for the table to be visible\n",
    "    table = wait_for_element_visibility(driver, By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]')\n",
    "\n",
    "    table_html = table.get_attribute('outerHTML')\n",
    "    # Convert the HTML table to a DataFrame\n",
    "    df = pd.read_html(io.StringIO(table_html))[0]\n",
    "\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df[\"District\"] = \"Central\" \n",
    "    df[\"SRO_NAME\"] = \"Sub Registrar I-Kashmere Gate\" \n",
    "    # Remove the last two rows\n",
    "    df = df.iloc[:-2]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def scrape_all_pages(driver, num_pages, scrape_table):\n",
    "    final_data = scrape_table(driver)\n",
    "    max_attempts = math.ceil(num_pages/10)  # Calculate the maximum number of attempts needed to scrape all pages\n",
    "    \n",
    "    total_checks = [i for i in range(1, max_attempts + 1)]  # Create a list of total checks\n",
    "    \n",
    "    # Loop through each attempt\n",
    "    for i in range(1, max_attempts + 1, 1): \n",
    "        # Check if it's the first or last attempt\n",
    "        if i ==1 : \n",
    "            # Loop through each page element (from 2 to 11)\n",
    "            for j in range(2, 12, 1):\n",
    "                # Scroll to the bottom of the page\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(3)\n",
    "                # Construct the XPath to find the next page element\n",
    "                next_element_path = f'//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]/tbody/tr[17]/td/table/tbody/tr/td[{j}]/a'\n",
    "                # Find the next page element by its XPath\n",
    "                next_page_element = WebDriverWait(driver, 10).until(\n",
    "                   EC.presence_of_element_located((By.XPATH, next_element_path))\n",
    "                )\n",
    "                # Click on the next page element\n",
    "                actions = ActionChains(driver)\n",
    "                actions.click(next_page_element).perform()\n",
    "                # Wait for the table to appear\n",
    "                wait_for_element_visibility(driver, By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]')\n",
    "                # Scrape data from the current page\n",
    "                temp_df = scrape_table(driver)\n",
    "                # Concatenate the scraped data with the final data\n",
    "                final_data = pd.concat([final_data, temp_df])\n",
    "        elif  total_checks[-1] == i: \n",
    "\n",
    "               for m in range(4, 12, 1):\n",
    "                # Scroll to the bottom of the page\n",
    "                  driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                  time.sleep(3)\n",
    "                # Construct the XPath to find the next page element\n",
    "                  next_element_path = f'//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]/tbody/tr[17]/td/table/tbody/tr/td[{m}]/a'\n",
    "                # Find the next page element by its XPath\n",
    "                  next_page_element = WebDriverWait(driver, 10).until(\n",
    "                   EC.presence_of_element_located((By.XPATH, next_element_path))\n",
    "                  )\n",
    "                # Click on the next page element\n",
    "                  actions = ActionChains(driver)\n",
    "                  actions.click(next_page_element).perform()\n",
    "                # Wait for the table to appear\n",
    "                  wait_for_element_visibility(driver, By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]')\n",
    "                # Scrape data from the current page\n",
    "                  temp_df = scrape_table(driver)\n",
    "                 # Concatenate the scraped data with the final data\n",
    "                  final_data = pd.concat([final_data, temp_df])\n",
    "\n",
    "        else:\n",
    "            # Loop through each page element (from 3 to 12)\n",
    "            for k in range(3, 13, 1): \n",
    "                # Scroll to the bottom of the page\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                time.sleep(3)\n",
    "               \n",
    "                next_element_path = f'//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]/tbody/tr[17]/td/table/tbody/tr/td[{k}]/a'\n",
    "\n",
    "                # Find the \"...\" element by its text\n",
    "                next_page_element = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, next_element_path))\n",
    "                )\n",
    "                # Click on the \"...\" element using JavaScript\n",
    "                driver.execute_script(\"arguments[0].click();\", next_page_element)\n",
    "                # Wait for the table to appear\n",
    "                wait_for_element_visibility(driver, By.XPATH, '//*[@id=\"ctl00_ContentPlaceHolder1_grd_complaintdetail\"]')\n",
    "                # Scrape data from the current page\n",
    "                temp_df = scrape_table(driver)\n",
    "                # Concatenate the scraped data with the final data\n",
    "                final_data = pd.concat([final_data, temp_df])\n",
    "            time.sleep(5)\n",
    "        print(f\"Successfully scraped for full page {i}\")\n",
    "\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped for full page 1\n",
      "Successfully scraped for full page 2\n",
      "Successfully scraped for full page 3\n"
     ]
    }
   ],
   "source": [
    "num_pages = 29  # Number of pages to scrape\n",
    "\n",
    "# Call the scrape_all_pages function\n",
    "all_data = scrape_all_pages(driver, num_pages, scrape_table= scrape_page)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()  # No issues with closing the WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "num_pages =29\n",
    "max_attempts = math.ceil(num_pages/10)  # Calculate the maximum number of attempts needed to scrape all pages\n",
    "total_checks = [i for i in range(1, max_attempts + 1)]  # Create a list of total checks\n",
    "print(max_attempts, total_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_excel(\"C:\\\\Users\\\\niranjan.kumar\\\\Desktop\\\\Property_Tax\\\\Raw_Urban_Data\\\\Delhi_Dispute\\\\CENTRAL_DELHI.xlsx\", index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
